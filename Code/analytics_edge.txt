 \chapter{Analytics Methods}
\label{chap:Methods}

\thispagestyle{empty}                    
\begin{center}
{\it The value of an idea lies in the using of it.}\\
\vspace{0.25cm}
-- Thomas A. Edison \\
%{\it The goal is to turn data into information, and information into insight.} \\
%\vspace{0.25cm}
%-- Carly Fiorina, Former CEO of HP\\
\end{center}                  
\vfill
\inabox{\centerline{\bf Contents}
\texitem{\ref{chap:Methods}.1.} Linear Regression
\texitem{\ref{chap:Methods}.2.} Logistic Regression
\texitem{\ref{chap:Methods}.3.} Classification and Regression Trees (CART) and Random Forests
\texitem{\ref{chap:Methods}.4.} Clustering
\texitem{\ref{chap:Methods}.5.} Visualization
\texitem{\ref{chap:Methods}.6.} Linear and Integer Optimization
\texitem{\ref{chap:Methods}.7.} Notes and Sources
}
\vfill
\hfill
{\bf \arabic{page}}
\par
\eject


\noindent In this chapter, we give a brief overview of several analytics methods that were referenced throughout the book. We do not cover all of the analytics methods discussed, but we try to cover many different methods that can be used for descriptive, predictive, and prescriptive analytics. Tutorials and resources for recommended software options can be found in the Online Companion for this book (\url{www.dynamic-ideas.com/analytics-edge}). For a more technical and comprehensive understanding of the methods presented here, we give several references in Section \ref{sec:MethodsNotesSources}. 



\section{Linear Regression}
\label{sec:LinearRegression}


Linear regression is one of the most common methods for making predictions. It is used to determine how an outcome variable, called the \emph{dependent variable}, can best be expressed as a linear combination of a set of known input variables, called the \emph{independent variables}.  The dependent variable is typically denoted by $y$ and the independent variables are denoted by $x_1, x_2, \ldots, x_k$, where $k$ is the number of different independent variables. We are interested in finding the best possible coefficients $\beta_0, \beta_1, \beta_2, \ldots, \beta_k$ such that our predicted values: 

\vspace{-0.2cm}
\begin{center}
\noindent \fcolorbox{white}{white}{\parbox{4in}{{\small 
$$\hat{y} = \beta_0 + \beta_1x_1 + \beta_2x_2 + \ldots + \beta_kx_k$$
}}}
\end{center}
\vspace{-0.2cm}


\noindent are as close as possible to the actual $y$ values. There are many different ways to define the ``best'' coefficients. For basic linear regression, this is achieved by minimizing the sum of the squared differences between the actual values, $y$, and the predictions $\hat{y}$. These differences, $(y - \hat{y})$, are often called {\it error terms} or {\it residuals}. 

As an example, we saw in Chapter 1 that Orley Ashenfelter developed the following multiple linear regression equation to predict wine quality:

\begin{center}
\smallskip
\noindent \fcolorbox{white}{white}{\parbox{3.75in}{{\small 
Wine Quality = -12.145 + (0.001173 $\times$ Winter Rainfall) + (0.616 $\times$ Average Growing Season Temperature) - (0.00386 $\times$ Harvest Rainfall) + (0.0238 $\times$ Age of Vintage).
}}}
\smallskip
\vspace{0cm}
\end{center}

\noindent Here, the dependent variable is ``Wine Quality" and the four independent variables are ``Winter Rainfall," ``Average Growing Season Temperature," ``Harvest Rainfall" and ``Age of Vintage." By using a data set and minimizing the sum of the squared differences between the actual values and the predicted values, Ashenfelter discovered that the best coefficients were $\beta_0 = - 12.145$, $\beta_1 = 0.001173$, $\beta_2 = 0.616$, $\beta_3 = -0.00386$ and $\beta_4 = 0.0238$. 


\subsection*{Evaluating a Model}

Once a linear regression model is constructed, it is important to evaluate the quality of the model. When a linear regression model is created with a statistical package, a lot of additional information is typically generated about the model in addition to the coefficient estimates, or the $\beta$ values. 

\subsubsection*{Significance}

One important piece of information to consider when evaluating the model is the significance of the independent variables. In a linear regression model, a coefficient is considered {\it significant} if the coefficient estimate is significantly different from zero according to the data used to build the model. A coefficient of zero means that the value of the independent variable does not change the prediction for the dependent variable. If a coefficient is not significantly different than zero, then we should probably remove the variable from the model, since it is not helping predict the dependent variable. 

 More specifically, a {\it standard error} value can be computed for each coefficient. This value gives a measure of how much the ``true'' coefficient is likely to vary from the estimate value, given the data used to build the model. Keep in mind that the standard error only holds for the data used to build the model. If the dataset is not representative of future data, the model might not perform as well. 

A {\it t-value} for a coefficient is also often computed, which is the coefficient estimate divided by the standard error (it will be negative if the coefficient is negative, and positive if the coefficient is positive). A larger absolute $t$-value means that the coefficient is more significant (we want independent variables with a small standard error relative to the coefficient estimate). So we ultimately want independent variables with large absolute $t$-values. A probability that a coefficient is actually zero given the data can also be computed (called a $p$-value), and the linear regression model output in any software package will often make it easy to tell which variables are significant. For a more mathematically rigorous treatment of these topics, see the references in Section \ref{sec:MethodsNotesSources}.


\subsubsection*{Removing Independent Variables}

If there are insignificant independent variables in the model, we should consider removing these variables, for several reasons. One is that having too many independent variables compared to the number of data observations can cause the model to {\it overfit} to the data used to build the model. This leads to a model that does really well at predicting the outcome for data it has seen before, but really poorly at predicting the outcome for new observations. 

If there is significantly more data than independent variables, then overfitting is not really a concern, and leaving in independent variables that are insignificant will not hurt the predictive ability of the model. But it might still be better to remove the insignificant variables for another important reason: simplicity and interpretability of the model. A model with unnecessary independent variables requires more data, and makes the model more complex than it needs to be. Simpler models that are just as accurate should be preferred to more complex models that do not improve the accuracy of prediction. However, a modeler should be careful when removing independent variables because there are interactions going on between the independent variables that can change when a variable is removed. We discuss this more below. 

\subsubsection*{The Metric $\boldsymbol{R^2}$}


Another important evaluation of a linear regression model is to compute the R-squared, or $R^2$, value. The $R^2$ of a linear regression model can be computed with the following equation: 

\begin{center}
\noindent \fcolorbox{white}{white}{\parbox{3in}{{
\begin{equation*}
\displaystyle
R^2 = 1 - \frac{\sum\limits_{i = 1}^{n} (y_i - \hat{y_i})^2}{\sum\limits_{i = 1}^{n} (y_i - \bar{y})^2},
\end{equation*}
}}}
\end{center}
\vspace{-0.2cm}

\noindent where $n$ is the number of data observations, $y_i$ is the actual value of the dependent variable for observation $i$, $\hat{y_i}$ is the predicted value of the dependent variable for observation $i$, and $\bar{y}$ is the mean value of the dependent variable, for all observations. The numerator of the fraction is called the {\it sum of squared errors (SSE)} and the denominator of the fraction is called the {\it total sum of squares (SST)}. 

The $R^2$ value can be interpreted as the proportion of the total variation of the dependent variable that is accounted for by the independent variables used in the regression equation. A different interpretation of $R^2$ is that it measures the value added of the model over a ``baseline'' model of predicting the average of the dependent variable for all observations. It is unit-less and takes values between 0 and 1. The worst case is an $R^2$ of 0, because it means that using the independent variables to make predictions is no better than just predicting the mean of the dependent variable for all observations. The best case is an $R^2$ of 1, because it means that the model perfectly predicts the variation of the dependent variable, and we do not make any errors. 

Generally, a higher $R^2$ value implies a better regression model, but this can be very dependent on the application area. A valuable model for a hard problem (like predicting stock prices) can have a low $R^2$ because the problem is so challenging, and a less valuable model for an easy problem can have a high $R^2$ because the problem is so easy (like predicting revenue of a product using the number of items sold). Therefore, it is important to evaluate the use of a model in practice in addition to computing the $R^2$ value.



\subsection*{Refining the Model}

If it has been determined that one or more insignificant independent variables should be removed from the model, it is typically easy to rebuild the model without a few independent variables. However, the independent variables should be removed one at a time. This is due to interactions between the independent variables, specifically something called {\it multicollinearity}. Multicollinearity means that two independent variables are highly correlated. If there is multicollinearity in the model, two independent variables are basically representing the same thing. This can make the variables seem insignificant when they actually should be significant. By removing just one of them, it can be discovered that the other variable is significant in the model.

If there are multiple insignificant independent variables in the model, a general rule for which variable to remove first is to remove the one that is the least significant. However, expert human judgment can also be a good strategy for deciding which of two insignificant variables to remove first, based on intuition about the model.  

\subsection*{Making Predictions}

An important final step to validate a linear regression model is to make predictions on new data, which is often called {\it out-of-sample} data or a {\it test set}. If a linear regression model is not good at predicting unseen data, it might be overfit to the training data. 

To make predictions, the new data should be plugged into the linear regression equation. For example, recall Ashenfelter's linear regression equation to predict the quality of wine:

\begin{center}
\smallskip
\noindent \fcolorbox{white}{white}{\parbox{3.75in}{{\small 
Wine Quality = -12.145 + (0.001173 $\times$ Winter Rainfall) + (0.616 $\times$ Average Growing Season Temperature) - (0.00386 $\times$ Harvest Rainfall) + (0.0238 $\times$ Age of Vintage).
}}}
\smallskip
\vspace{0cm}
\end{center}

\noindent Suppose we have two new data points about wine prices, which Ashenfelter did not use to build his model, described in Table \ref{tab:WineTest}. 

\begin{table}[b!]
\caption{Out-of-sample data for Ashenfelter's wine equation.}
\begin{center}
\begin{tabular}{>{\raggedright}p{1.5in}>{\centering}p{1.25in}>{\centering}p{1.25in}}
{\bf Variable} & {\bf Test Point 1} & {\bf Test Point 2} \tabularnewline
\cmidrule[1pt]{1-3}
Winter Rainfall & 717 & 578 \tabularnewline
\cmidrule[0.5pt]{1-3}
Average Growing Season Temperature & 16.17 & 16.00 \tabularnewline
\cmidrule[0.5pt]{1-3}
Harvest Rainfall & 122 & 74 \tabularnewline
\cmidrule[0.5pt]{1-3}
Age of Vintage & 4 & 3 \tabularnewline
\end{tabular}
\label{tab:WineTest}
\end{center}
\end{table}

Using the linear regression equation, we can predict that for Test Point 1,

\begin{center}
\smallskip
\noindent \fcolorbox{white}{white}{\parbox{3.75in}{{\small 
Wine Quality = -12.145 + (0.001173 $\times$ 717) + (0.616 $\times$ 16.17) - (0.00386 $\times$ 122) + (0.0238 $\times$ 4) = -1.719,
}}}
\smallskip
\vspace{-0.2cm}
\end{center}

\noindent and for Test Point 2,

\begin{center}
\smallskip
\noindent \fcolorbox{white}{white}{\parbox{3.75in}{{\small 
Wine Quality = -12.145 + (0.001173 $\times$ 578) + (0.616 $\times$ 16.00) - (0.00386 $\times$ 74) + (0.0238 $\times$ 3) = -1.825.
}}}
\smallskip
\vspace{-0.2cm}
\end{center}


\noindent Note that we do not need the values of the dependent variables to make predictions. This is important, because often it is necessary to make future predictions before the results are realized (for example, think about predicting the winner of the Academy Awards the week before the results are revealed). But suppose that we know that the actual Wine Quality of the first data point is -1.540, and the actual Wine Quality of the second data point is -1.996. Then we can compute our accuracy on this out-of-sample data by computing the test set $R^2$. 

First, we can compute the sum of squared errors:
$$SSE = (-1.719 - (-1.540))^2 + (-1.825 - (-1.996))^2 = 0.061.$$

\noindent Then we can compute the total sum of squares:
$$SST = (-1.426 - (-1.540))^2 + (-1.426 - (-1.996))^2 = 0.338,$$

\noindent where -1.426 is the average wine quality of the data used to build the model. Note that we use the mean of Wine Quality in the {\it training set} to calculate SST. This is because we are trying to compare ourselves against how well we could do by just guessing the mean. In reality, we often have to make out-of-sample predictions before we know the outcome, and so we can not use the test set mean. To make this calculation match what will occur in practice, we should use the training set mean. (Note that this convention makes it so that the $R^2$ of a model on the test set can sometimes be negative!)

So the $R^2$ on our test set is 
$$ R^2 = 1 - \frac{0.061}{0.338} = 0.82 $$

\noindent This is promising, but we would want to get a larger test set to be more confident of this result. Computing how good a model is at making out-of-sample predictions is crucial for evaluating the predictive power of the model. 






\section{Logistic Regression}
\label{sec:LogisticRegression}


 Logistic regression is a predictive method that handles cases where the dependent variable, $y$, only has two possible outcomes, called {\it classes}. Examples of dependent variables that could be used with logistic regression are predicting whether a new business will succeed or fail, whether or not a loan will be approved, and whether a team will win or lose a game. These are all called {\it classification} problems, since the goal is to figure out which class each observation belongs to. 

Similar to linear regression, logistic regression uses a set of independent variables to make predictions, but instead of predicting a continuous value for the dependent variable, it predicts the probability of membership in each of the possible outcomes, or classes. 

The two possible outcomes are often denoted as 1 and 0. For example, if our possible outcomes were ``success" or ``failure," then we could denote success by 1 and failure by 0. If our possible outcomes were ``yes" or ``no," then we could denote yes by 1 and no by 0. We will refer to the two possible classes as 1 and 0 for the rest of this section.

 Logistic regression consists of two steps. The first step is to compute the probabilities of the two different outcomes. Thus, for each observation $i$, we are computing $P(y_i = 1)$, or the probability the observation belongs to class 1, and $P(y_i = 0)$, or the probability the observation belongs to class 0. Since we only have two possible outcomes, we know that $P(y_i = 0) = 1 - P(y _i= 1)$ according to the basic properties of probability (the sum of the probabilities for all possible outcomes must equal 1). So logistic regression with two classes only needs to predict $P(y_i = 1)$ for each observation $i$, since we can easily compute $P(y_i = 0)$. 


In the second step of logistic regression, we use a {\it cutoff}, or {\it threshold}, value to classify each observation into one of the classes. A common cutoff value is 0.5, meaning that if $P(y_i = 1) \geq 0.5$ we will classify observation $i$ into class 1 and if $P(y = 1) < 0.5$ we will classify observation $i$ into class 0. Simply, we will classify each observation into the class with the highest probability. However, other cutoff values can be chosen, and in some cases are more appropriate. For example, if the probability of belonging to class 1 is a low probability event, a lower cutoff value might be sufficient. We discuss this more later in this section.

\subsection*{The Logistic Regression Model}



The probabilities in logistic regression are computed using an equation that is similar to the equation used for linear regression. In linear regression, our equation to predict $y$ was $y = b_0 + b_1x_1 + b_2x_2 + \ldots + b_kx_k$. Since in logistic regression we want our prediction to be a probability, we use a nonlinear function that will only produce values between 0 and 1:

\smallskip
\noindent \fcolorbox{white}{white}{\parbox{\textwidth}{{
$$P(y = 1) = \frac{1}{1 + e^{-(b_0 + b_1x_1 + b_2x_2 + \ldots + b_kx_k)}}$$
}}}
\smallskip
\vspace{-0.3cm}

\noindent This is called the \emph{logistic response function} and it predicts the probability of an observation belonging to class 1. For more technical details about the logistic response function, see the references in Section \ref{sec:MethodsNotesSources}.   



 To make this equation easier to understand, we define what is known as the \emph{odds} of belonging to class 1:

\smallskip
\noindent \fcolorbox{white}{white}{\parbox{\textwidth}{{
$$Odds = \frac{P(y = 1)}{P(y = 0)} = \frac{P(y = 1)}{1 - P(y = 1)}.$$
}}}
\smallskip
\vspace{-0.3cm}

\noindent This metric is very popular in gambling games and many other areas. Instead of talking about the probability of an outcome, we can refer to the odds of an outcome. When the probability of class 1 is equal to the probability of class 0, the odds equal one. When the probability of class 1 is larger than the probability of class 0, the odds are larger than one, and when the probability of class 1 is smaller than the probability of class 0, the odds are smaller than one. So the odds capture how likely outcome 1 is to ``win'' over outcome 0. If we substitute the logistic response function into this equation for $P(y = 1)$, we have (after some algebraic manipulation):

\smallskip
\noindent \fcolorbox{white}{white}{\parbox{\textwidth}{{
$$Odds = e^{b_0 + b_1x_1 + b_2x_2 + \ldots + b_kx_k}.$$
}}}
\smallskip
\vspace{-0.3cm}

\noindent By taking the logarithm of both sides we get that:

\smallskip
\noindent \fcolorbox{white}{white}{\parbox{\textwidth}{{
$$log(Odds) = b_0 + b_1x_1 + b_2x_2 + \ldots + b_kx_k,$$
}}}
\smallskip
\vspace{-0.3cm}

\noindent where the right hand side looks like our linear regression equation. The $log(Odds)$ is called the {\it Logit}, and it is modeled like the dependent variable in a linear regression equation. Sometimes this equation is easier to understand, because it is more clear how the coefficients affect the Logit. The interpretation of the coefficients for the Logit is the same as that for linear regression. If the coefficient of an independent variable is positive, then it leads to a larger logit value (assuming the independent variable takes positive values), which in turn leads to higher Odds, or a higher probability of class 1. On the other hand, if the coefficient of an independent variable is negative (assuming the independent variable takes positive values), then it leads to a smaller logit value, or a lower probability of class 1. 


\subsection*{Selecting a Cutoff Value}



The logistic response function predicts the probability of outcome 1. Sometimes this probability is useful, since it gives a measure of confidence in the prediction. For example, if we predicted the probability of belonging to class 1 for one observation to be 0.99, and for another observation to be 0.6, we are more confident for the first observation that class 1 is the correct class.

However, logistic regression is most often used to classify each of the data points into one of the classes. To do this, a cutoff or threshold value needs to be selected. Once a threshold value is selected, all observations with probabilities above this value are classified into class 1, and all observations with probabilities below this value are classified into class 0.

Different threshold values can yield different amounts of error in the predictions. In a classification problem, an error occurs when we classify an observation into one class, but it actually belongs to a different class. The accuracy of the model is the number of observations that the model classifies correctly. Most of the time, a model will make some errors, and for this reason there are several approaches to determining the appropriate threshold value. A popular default value is 0.5, with the rationale that an observation should be classified into the class for which its probability is the highest. However, a different threshold value can be selected, because one type of error is seen as worse than the other. Whenever the probabilities are converted to class predictions, there are two types of errors that can be made: {\it false positives} and {\it false negatives}. A false positive error is when the model predicts class 1, but in reality the observation belongs to class 0. A false negative error is when the model predicts class 0, but in reality the observation belongs to class 1. If a higher threshold is selected, more false negative errors will be made, because class 1 will only be predicted if the probability is really high. If a lower threshold is selected, more false positive errors will be made, because class 1 will be predicted more often.

Let us think about this in terms of some applications. One application where decision-makers often have an error preference is in disease prediction. Suppose a model is built to predict whether or not someone will develop heart disease in the next 10 years (like the model we saw in Chapter \ref{chap:Framingham} about the Framingham Heart Study). We will consider class 1 to be the outcome in which the person does develop heart disease, and class 0 the outcome in which the person does not develop heart disease. If a high threshold is selected, false negative errors are more likely, which means that the model predicts that the person would not develop heart disease, but they actually did. If a lower threshold is selected, false positive errors are more likely, which means that the model predicts the person will develop heart disease, but they actually did not. In this example, a false positive error is often preferred by the decision-maker. Unnecessary resources might be spent treating a patient who did not need to worry, but fewer patients go untreated (which is what a false negative error does).

Now, consider spam filters. Almost every email provider has a built in spam filter that tries to detect whether or not an email message is spam. We can classify spam messages as class 1 and non-spam messages as class 0. Then, if we build a logistic regression model to predict spam, we might want to select a high threshold. Why? In this case, a false positive error means that we predicted a message was spam, and sent it to the spam folder, when it actually was not spam. We might have just sent an important email to the junk folder! On the other hand, a false negative error means that we predicted a message was not spam, when it actually was. This creates a slight annoyance for the user (since they have to delete the message from the inbox themselves) but at least an important message was not missed. Additionally, each time a user deletes a spam message, many email providers allow them to mark it as spam, so the system can learn from its mistakes.


We can formalize this error trade-off with a Receiver Operator Characteristic curve, or ROC curve. An ROC curve plots the false positive rate of a model on the x-axis, and the true positive rate (the percentage of class 1 observations that the model classified correctly) of the model on the y-axis for different threshold possibilities. An example ROC curve is shown in Figure \ref{fig:ExampleROC}. 

\begin{figure}[b!]
	 \caption{An example ROC curve for a logistic regression model.}
	\center{\includegraphics[width=4in]{framinghamROC_v2.pdf}}
	\label{fig:ExampleROC}
\end{figure} 

The line shows how the false positive rate and true positive rate vary with different threshold values for a particular model. A lower threshold corresponds to values in the top right of the plot, since by selecting a lower threshold we predict an outcome of 1 more often. This allows us to maximize our true positive rate, but it also increases the false positive rate. A higher threshold corresponds to values in the bottom left of the plot.

Ideally, we want the ROC curve to be as close to the top left corner of the plot as possible, since it gives a high true positive rate and a low false positive rate. This motivates an important metric for classification problems: the AUC, or area under the curve. The AUC of a model gives the area under the ROC curve, and is a number between 0 and 1. The higher the AUC, the more area under the ROC curve, and the better a model is at distinguishing between the two different classes. The AUC of a model can be interpreted as follows: if a model were handed two random observations from the dataset used to build the ROC curve, one belonging to one class and one belonging to the other class, the AUC gives the percentage of the time that we expect the model to predict the two observations correctly. If we were to just guess which observation was which, this would be an AUC of 0.5. (The ROC curve in this case would be a straight line from the bottom left corner of the plot to the top right corner of the plot.) So a model with an AUC greater than 0.5 is doing something smarter than just guessing, but we want the AUC of a model to be as close to 1 as possible.


Once a logistic regression model is constructed and a threshold value is selected, the model should be tested on out-of-sample data. Similarly to linear regression, this can be done by plugging the new data into the logistic response function to get probability predictions, and then using the threshold value to get class predictions. To evaluate the model's predictive ability, the accuracy of the model, error rates, and AUC can be computed on the test set.

In the next section, we discuss two additional methods that can be used for both classification and regression.



% Do we need to discuss splitting into training and testing sets here?



\section{CART and Random Forests}
\label{sec:CART}


Classification and regression trees (CART) and random forests are both tree-based methods. These are flexible data-driven methods for determining an outcome using splits, or logical rules, on the independent variables. Unlike linear and logistic regression, we do not need to assume that the outcome variable depends linearly on the independent variables. CART and Random Forests can be used for both a continuous outcome (like in linear regression) and a categorical outcome (like in logistic regression).

\subsection*{CART}

The simpler of the two methods is CART. In a CART model, a single tree is constructed, and for this reason, CART models are very easy to interpret, and are popular for applications where the method needs to be easy to explain. This interpretability and the ability to capture nonlinearities in the data are two major benefits of CART. 

As a small example, suppose we want to classify whether applicants to a job position will be given an interview. To construct the tree, we start with the training data provided in Table \ref{CARTexampleData}. 

There are currently 5 applicants who have already been informed of whether or not they will receive an interview, and two independent variables that we will use to build our model: highest level of education and previous experience in the field. Based on this data, we can use CART to build the tree shown in Figure \ref{fig:SimpleCART}. 

\begin{table}[b!]
\caption{Data for a small set of job applicants.}
\vspace{0.25cm}
\label{CARTexampleData}
\begin{center}
\begin{tabular}{lccc}
{\bf Name} & {\bf Highest Level of Education} & {\bf Experience} & {\bf Interview} \\
\cmidrule[1pt]{1-4}
Andrew & MBA degree & Yes & Yes \\ 
Betty & Some College & Yes & No \\ 
Charlie & Bachelor's degree & No & No \\ 
Diane & Bachelor's degree & Yes & Yes \\ 
Edward & PhD & No & No \\ 
\end{tabular}
\end{center}
\end{table}

\begin{figure}[h!]
	 \caption{CART tree for the job application example.}
	\center{\includegraphics[width=3.5in]{SimpleCART}}
	\label{fig:SimpleCART}
\end{figure}

This tree correctly labels all of the given data, and can be used to predict whether or not a new applicant will be given an interview by following the splits down the tree, from top to bottom. For example, if an application is received from Fred who does not have previous experience, we predict that he will not receive an interview. If another application is received from Gloria who has previous experience and a Master's degree, we predict that she will receive an interview. Note that it could also be the case that the split on the highest level of education should exclude people with too much education; it could be that for this particular position, anything above a Master's degree would make the candidate over-qualified. By using a CART tree, this type of nonlinear relationship could easily be detected.

Each split in a CART tree is always based on a single independent variable, and a CART tree can be given all available independent variables. If a particular independent variable is decided to not be significant, the CART tree will not use that variable at all to make a split. For this reason, a CART tree is often useful to find the significant variables when tens, or even hundreds, of independent variables are available.

The splits used in a CART model divide the observations into smaller groups, called {\it leaves} or {\it buckets}. In this example, all of the applicants without previous experience were placed into one bucket, all of the applicants with previous experience but without a bachelor's degree or higher were placed into a second bucket, and all of the applicants with previous experience and at least a bachelor's degree were placed into a third bucket. These splits are selected to make the buckets as homogenous or ``pure" as possible, meaning that we want each of the buckets to contain observations belonging to just one class. This is not always possible, since we might have two observations with exactly the same values of the independent variables, but different dependent variable values. For example, we could have two applicants with previous experience and MBA degrees, but one received an interview and the other did not. There is no possible split on our two independent variables that would put these observations into different buckets.

There are a number of ways to measure how pure a split is, including what are known as the {\it Gini index} and an {\it entropy measure}. We will not describe the specifics of these methods here, but the main idea is that they give a higher ``purity" measure to a bucket with a higher proportion of observations belonging to the same class.

Even if it is possible to divide the observations into pure buckets, it may not be a good idea. It could lead to overfitting of the training data, meaning that the model will not extend well to data that it has never seen before. One popular method to prevent overfitting from happening is called ``pruning" the tree. In this approach, a complete tree is constructed to make the buckets as pure as possible, and then certain branches are deleted if they do not reduce the impurity of the buckets by a high enough value. Other approaches are to set parameter values to limit the number of splits in the tree, to set a minimum number of observations that must be in a group before a split is attempted, or to set a minimum number of observations that must be in each final group of the tree. 


The choice of these parameters can often influence the accuracy of the model, and several different parameter choices should be tested. One way of doing this is with a technique called {\it cross-validation}. Another is by creating a {\it validation} set of data. Then, for several different parameter values, a model is built using the training set, and its accuracy is assessed on the validation set. Whichever parameter choice performs the best on the validation set should be selected for the final model. Then, the performance of the final model is assessed on the test set. The validation set is needed so that the test set remains a true out-of-sample test. If the test set is used to select the parameter value, the test set was used to build the model and no longer represents how the model will perform on new data. 

For each bucket or leaf of a CART tree, the typical prediction is to predict the majority outcome of all of the observations in the training set that belong in that bucket (assuming we have a classification problem). We can instead assign a probability to each observation of belonging to each class by computing the percentage of observations in the corresponding bucket that belong to each class. Then these probabilities can be thresholded to capture different error preferences, similarly to logistic regression (note that predicting the majority outcome is like using a threshold of 0.5). 

CART can easily be used to predict an outcome with more than two classes, or a continuous outcome. With a continuous outcome, the prediction for each group is the average value of the dependent variable over all training points that were classified into that group. In a CART model, regardless of the outcome type, keep in mind that all observations that are classified into the same group, or bucket, will have the same prediction.

After building a CART model, a couple of steps should be taken to assess the validity and evaluate the performance of the tree:

\begin{enumerate}
\item Plot the tree and look at each of the decision rules. Check to see if they make sense intuitively. Rules that seem strange could still be accurate, or it could be a sign that the data needs to be examined. This is also a good way to see which independent variables are the most important for the model, by seeing which ones were used in the tree.
\item Assess the accuracy of the tree on a test set. The predictions should be compared to the actual values by using a classification matrix (for a categorical outcome) or by computing the $R^2$ value (for a continuous outcome). 
\end{enumerate}

CART often performs well across a wide range of situations, requires little effort from the analyst, and is easily understandable by the user of the model. It can also be used to detect which variables out of a large number of possible choices are the most important, since the most significant independent variables will appear at the top of the tree. CART models work best when there are large amounts of data, but a variation of this method, called random forests, was designed to improve the performance of trees on smaller datasets.


\subsection*{Random Forests}

The method of random forests, developed by Leo Breiman and Adele Cutler, was designed to improve the prediction accuracy of CART. It works by building a large number of CART trees which each ``vote'' on the outcome to make a final prediction. Unfortunately, this makes the method less interpretable than CART, so often the modeler needs to decide which is more important: the interpretability of the model, or attaining the maximum possible accuracy. 

A random forest builds many trees by randomly selecting a subset of observations and a subset of independent variables to use for each tree. We could not just run CART many times, because we would get the same tree every time. Instead, each tree is given a random subset of the independent variables from which it can select the splits, and a training set that is a {\it bagged} or {\it bootstrapped} sample of the full dataset. This means that the data used as the training data for each tree is selected randomly with replacement from the full dataset. As an example, if we have five data points in our training set, labeled $\{1,2,3,4,5\}$, we might use the five observations $\{3,2,4,3,1\}$ to build the first tree, the five observations $\{5,1,2,3,2\}$ to build the second tree, etc. Notice that some observations are repeated, and some are not included in certain training sets. This gives each tree a slightly different training set, and helps make each tree see different things in the data. 

Just like in CART, there are some parameters that need to be selected for a random forest model. The first is the number of trees to build, which is typically set to a few hundred. The more trees that are built, the longer the model will take to build, but if not enough trees are built, the full potential of the model might not be realized. Another parameter is needed to control the size of the trees. A popular choice is to set a minimum number of observations that can be in a bucket, or subset, of a tree. This is the same as one of the parameters that can be used in a CART model. Random forest models have been shown to be less sensitive to the parameters of the model, so it is typically sufficient  to set the parameters to reasonable values. To test different parameter settings, the same validation approach described for building a CART model can be used here.

Unfortunately, it can be hard to interpret a random forest model because it consists of a large number of trees. For this reason, it is typically viewed as more of a ``black box'' method, meaning that it is not easily interpretable. If interpretability and understanding the model are key objectives for a particular application, then a random forest model is probably not the best model to use.



\section{Clustering}
\label{sec:Clustering}



Clustering is different from the other analytics methods we have discussed so far in this chapter, in that it is an {\it unsupervised} learning method. This means that the goal of clustering is not to predict an outcome, or dependent variable. It is instead to segment a set of observations into similar groups, based on the available data. Figure \ref{fig:ClusteringExample} gives an example of how a set of points can be clustered into three groups, just using the distance between the points. The points are assigned to a group based on how close together they are. 

\begin{figure}[b!]
	 \caption{Example of how points can be clustered into groups.}
	\center{\includegraphics[width=2in]{ClusteringExample}}
	\label{fig:ClusteringExample}
\end{figure} 

Although clustering is not designed to predict an outcome variable, clustering can still be useful to improve the accuracy of predictive models. The data can be clustered into similar groups, and then used to build a predictive model for each group. We saw an example of this in Chapter 8, ``Medical Monitoring and Predictive Diagnosis.'' A word of caution: this tends to work best for large datasets, since it is easy to overfit a model to the training set using this technique. 

There are many different algorithms for clustering, which differ in how the clusters are built and the properties of the clusters produced from the method. In this chapter, we will cover Hierarchical clustering and $k$-means clustering, two of the most popular clustering methods. In Chapter \ref{chap:Fraud}, a third clustering method is presented, called Condorcet Clustering. 


\subsection*{Distance}

Before a clustering algorithm can be applied, the distance metric that will be used to define the distance between two points and the distance between two clusters needs to be selected. 

The most popular and standard way to compute the distance between two points is Euclidean distance, which is defined as follows for data points {\bf x} and {\bf y}:

\smallskip
\noindent \fcolorbox{white}{white}{\parbox{\textwidth}{{\small 
$$d({\bf x}, {\bf y}) = \sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2 + (x_3 - y_3)^2 + \ldots + (x_k - y_k)^2}.$$
}}}
\smallskip
\vspace{-0.2cm}

\noindent For each attribute of the data points (here we are assuming there are $k$ attributes), Euclidean distance computes the difference between the two data points and squares it. Then these values are added across all attributes and the square root is taken. 

In addition to Euclidean distance, there are many other distance metrics that could be used. One is called {\it Manhattan distance}, which computes the distance between two points as the sum of the absolute values of the differences between the attributes, instead of the sum of squares. Another is called {\it maximum coordinate distance}, which defines the distance as the largest difference between the two data points in any attribute. In the rest of this chapter, we will use Euclidean distance. For an example of a non-Euclidean distance metric, see Chapter \ref{chap:Fraud}.

Another important distance that we have to define for clustering is the distance between two clusters, or groups of points. One way of doing this is called {\it minimum distance}, which defines the distance between two clusters as the distance between the data points in the clusters that are the closest together. Figure \ref{fig:ClusteringMinimumDistance} shows how we would define the distances between each cluster in our example. Each black line gives the distance between a pair of clusters. 

\begin{figure}[h!]
	 \caption{Example of minimum distance between clusters.}
	\center{\includegraphics[width=2.5in]{ClusteringMinimumDistance}}
	\label{fig:ClusteringMinimumDistance}
\end{figure} 

As we saw with the distance between two points, there are also several ways of defining the distance between two clusters. We could instead use {\it maximum distance}, which defines the distance between two clusters as the distance between the points that are the farthest apart. Another option is {\it centroid} distance, which defines the distance between two clusters as the distance between the cluster centroids.The centroid of a cluster is the point that is the average (in each attribute) of all of the data points in the cluster.  Figure \ref{fig:ClusteringCentroidDistance} shows the centroid of each cluster as a black square. The distance between two clusters would then be the distance between the two squares, or centroids. Note that each of these distances ultimately requires that we compute the distance between two points, for which we can use Euclidean distance.

\begin{figure}[b!]
	 \caption{Example of centroid distance between clusters.}
	\center{\includegraphics[width=2.5in]{ClusteringCentroidDistance_v2}}
	\label{fig:ClusteringCentroidDistance}
\end{figure} 

Distance can be highly influenced by the scale of the different attributes, so it is important to consider normalizing the data before computing distances. For example, suppose that the observations are companies. One of the attributes could be annual revenue, in dollars, and another attribute could be age of the company, in years. The revenue variable would dominate in the distance calculation because it is much larger than the years variable. The variables can be normalized to be on the same scale by subtracting the mean and dividing by the standard deviation (for each variable). 



\subsection*{Hierarchical Clustering}




In this section, we will discuss {\it agglomerative hierarchical clustering}, which starts with each observation in its own cluster. So if there are $n$ data points, or observations, it starts with $n$ clusters. Then, the distance between each pair of clusters (or points) is computed, and the two closest clusters are combined (resulting in $n-1$ clusters). This is repeated until all of the data points are in a single cluster. (A different form of hierarchical clustering, called divisive hierarchical clustering, starts with all of the points in a single cluster and iteratively splits the cluster into two pieces. We will be referring to the agglomerative form of hierarchical clustering throughout this section.)

This clustering process is called {\it hierarchical} because the clusters are nested. When two clusters are combined, the new cluster can be thought of as ``above'' the other clusters. This can be visualized with a {\it dendrogram}. A small example of the hierarchical clustering process and the resulting dendrogram is shown in Figure \ref{fig:ClusterDendrogram}. In this example, there are four data points shown on the left of the figure (colored red, green, blue, and black) and therefore the clustering process starts with four clusters. The two closest are the red and green points, so they are combined first. The next closest are the red/green cluster and the blue point, so they are combined next, and then lastly, the black data point is combined with the others. This process is shown in the dendrogram to the right of the points. Each of the points is shown on the bottom, and the process by which they are combined is shown with the gray lines. The height of the lines approximately shows how far apart the two clusters were when they were combined. 

\begin{figure}[b!]
	 \caption{Small example of hierarchical clustering and the resulting dendrogram.}
	\center{\includegraphics[width=4in]{ClusterDendrogram.pdf}}
	\label{fig:ClusterDendrogram}
\end{figure} 

Hierarchical clustering is done without first selecting the number of clusters desired. This is nice because the algorithm can be run without having to know the appropriate number of clusters for the problem. However, in many applications, a specific number of clusters is ultimately needed. This can be done after the hierarchical process is complete by looking at the dendrogram. Imagine a horizontal line cutting across the dendrogram. The number of vertical lines of the dendrogram that the horizontal line crosses shows how many clusters would be selected. That number of clusters tends to be a good choice if the horizontal line has a lot of ``wiggle room,'' meaning that the horizontal line can move up and down without running into a horizontal line of the dendrogram. Figures \ref{fig:ThreeClusters} and \ref{fig:TwoClusters} show the horizontal line and wiggle room for three clusters or two clusters. For this particular dataset, the dendrogram tells us that two clusters is a better choice, because the clusters were farther apart when they were combined (more wiggle room). However, when selecting the number of clusters for a particular problem, the decision-maker should also consider how many clusters make sense for the particular application.

\begin{figure}[b!]
	 \caption{Selecting the number of clusters using a dendrogram.}
	\center{\includegraphics[width=2.5in]{DendrogramThreeClusters.pdf}}
	\label{fig:ThreeClusters}
\end{figure} 

\begin{figure}[b!]
	 \caption{Selecting the number of clusters using a dendrogram.}
	\center{\includegraphics[width=2.5in]{DendrogramTwoClusters.pdf}}
	\label{fig:TwoClusters}
\end{figure} 




\subsection*{$K$-Means Clustering}




Instead of forming the clusters in a hierarchical way, $k$-means clustering iteratively re-assigns data points to clusters. First, the number of clusters desired has to be selected. It is often useful to run hierarchical clustering first to better understand which number of clusters would be good, or to think about the number of clusters that would be appropriate for the given problem. Alternatively, the clustering algorithm can be run several times with different choices for the number of clusters.

Once the number of clusters has been selected, each data point is randomly assigned to one of the clusters and each cluster centroid is computed. Then, each data point is re-assigned to the closest cluster centroid, and the cluster centroids are re-computed. This process repeats until no data point needs to be reassigned. It can also be stopped by defining a maximum number of iterations.

The largest benefit of the $k$-means clustering algorithm is that it is very fast, and works with small and large datasets. This is not true for hierarchical clustering, because of the distance computations required. Hierarchical clustering will not work if the dataset is too large, so in some situations, hierarchical clustering might not be an option. However, a dendrogram can not be used with the $k$-means clustering algorithm to assist in selecting the number of clusters. A different type of visualization, called a {\it scree plot}, can help in selecting the appropriate number of clusters for any clustering algorithm. For more information, see the references in Section \ref{sec:MethodsNotesSources}.

Regardless of the clustering algorithm used, the resulting clusters should be analyzed and explored to see if they are meaningful. This can be done by looking at basic statistics in each cluster, like the mean, minimum, and maximum values in each cluster and each variable. If the clusters have a feature in common that was not used in the clustering, like an outcome variable, then this often indicates that the clusters might help improve a predictive model.







\section{Data Visualization}
\label{sec:Visualization}



In this section, we will discuss the meaning of data visualization, why it is often useful to visualize data to discover hidden trends and properties, and a few different types of visualizations. 

Data visualization is defined as a mapping of {\it data properties} to {\it visual properties}. Data properties are usually numerical or categorical, like the mean of a variable, the maximum value of a variable, or the number of observations with a certain property. Visual properties are attributes like (x, y) coordinates, colors, sizes, or shapes. Both types of properties can be useful for understanding a dataset.

To motivate the need for data visualization, let us consider a famous example called ``Anscombe's Quartet.'' This example consists of four small datasets, each with two attributes. The data is given in Table \ref{tab:Anscombe}. Just looking at the data, it is hard to notice anything special about the different datasets. It turns out that the mean and standard deviation of the {$\mathbf X$} variable is the same for all four datasets, and the mean and standard deviation of the {$\mathbf Y$} variable is the same for all four datasets. Additionally, the correlation between {$\mathbf X$} and {$\mathbf Y$} is the same for all four datasets, and the regression equation to predict {$\mathbf Y$} from {$\mathbf X$} is even the same for all four datasets! So are these datasets very similar? 


\begin{table}[t!]
\caption{Data for Anscombe's Quartet.}
\begin{center}
\begin{tabular}{rr|rr|rr|rr}
{$\mathbf X_1$} & {$\mathbf Y_1$} & {$\mathbf X_2$} & {$\mathbf Y_2$} & {$\mathbf X_3$} & {$\mathbf Y_3$} & {$\mathbf X_4$} & {$\mathbf Y_4$} \\ 
\cmidrule[1pt]{1-8}
10.00 & 8.04 & 10.00 & 9.14 & 10.00 & 7.46 & 8.00 & 6.58 \\
8.00 & 6.95 & 8.00 & 8.14 & 8.00 & 6.77 & 8.00 & 5.76 \\
13.00 & 7.58 & 13.00 & 8.74 & 13.00 & 12.74 & 8.00 & 7.71 \\
9.00 & 8.81 & 9.00 & 8.77 & 9.00 & 7.11 & 8.00 & 8.84 \\
11.00 & 8.33 & 11.00 & 9.26 & 11.00 & 7.81 & 8.00 & 8.47 \\
14.00 & 9.96  & 14.00 & 8.10 & 14.00 & 8.84 & 8.00 & 7.04 \\
6.00 & 7.24 & 6.00 & 6.13 & 6.00 & 6.08 & 8.00 & 5.25 \\
4.00 & 4.26 & 4.00 & 3.10 & 4.00 & 5.39 & 19.00 & 12.50 \\
12.00 & 10.84 & 12.00 & 9.13 & 12.00 & 8.15 & 8.00 & 5.56 \\
7.00 & 4.82 & 7.00 & 7.26 & 7.00 & 6.42 & 8.00 & 7.91 \\
5.00 & 5.68 & 5.00 & 4.74 & 5.00 & 5.73 & 8.00 & 6.89 \\ 
\end{tabular}
\label{tab:Anscombe}
\end{center}
\end{table}

Figure \ref{fig:Anscombe} shows a scatterplot of each of the four datasets. By visualizing the data, we can see that they are actually very different. The first dataset (in the top left of the plot) looks like an appropriate dataset for linear regression, with a clear linear relationship between {$\mathbf X_1$} and {$\mathbf Y_1$}. The second dataset (in the top right of the plot) looks quadratic, and the third dataset (in the bottom left of the plot) looks perfectly linear with an outlier. The fourth dataset is perhaps the most unusual, with all of the points lying on a vertical line except for one. Without visualizing the data, we might not have noticed these differences, and we would have ignored valuable information that we could use when building a model.


\begin{figure}[h!]
	 \caption{Visualization of Anscombe's Quartet.}
	\center{\includegraphics[width=4in]{Anscombe.pdf}}
	\label{fig:Anscombe}
\end{figure} 


Beyond detecting patterns that are hard to notice by looking at the numerical data, visualization is also useful for interpreting a model and communicating results. Often, the person who built the model is not the person who will be implementing or using the model. With the correct visualizations, the task of describing a model or a dataset to the end user can be much easier. 

Throughout the rest of this section, we will describe a few different visualizations. All of the images shown here were created in R using the {\tt ggplot2} package. For more about this package and how to create these visualizations yourself, see the R Manual in the Online Companion. 

\subsection*{Scatterplots and Line Graphs}

Two basic visualizations are scatterplots and line graphs. While they are very simple ways of visualizing data, they can also be the cleanest and most understandable visualizations in some situations.

\subsubsection*{Scatterplots}


Scatterplots can be useful for quickly visualizing data trends in two dimensions. As an example, Figure \ref{fig:ggplotScatterplot} shows a basic scatterplot of per capita gross national income compared to fertility rate for all countries, according to the World Health Organization (WHO). Each point is a country, per capita gross national income is on the x-axis, and fertility rate is on the y-axis. 

\begin{figure}[h!]
	 \caption{Scatterplot of gross national income per capita versus fertility rate (number of children per woman) for all countries.}
	\center{\includegraphics[width=3in]{ggplotScatterplot.pdf}}
	\label{fig:ggplotScatterplot}
\end{figure} 

By coloring the points or changing the shapes, scatterplots can also be used to visualize data in more than two dimensions. Figure \ref{fig:ggplotScatterplot2} shows the same scatterplot, but now the points are colored by the region the points belong to. This plot easily allows us to see something we might not have observed before: The points from different regions are located in different areas of the plot.

\begin{figure}[h!]
	 \caption{Scatterplot of gross national income per capita versus fertility rate in the WHO dataset, with each point colored by the region it belongs to.}
	\center{\includegraphics[width=4in]{ggplotScatterplot2.pdf}}
	\label{fig:ggplotScatterplot2}
\end{figure} 



\subsubsection*{Line Graphs}


Line graphs are often very useful for visualizing data with a time component. For example, Figure \ref{fig:ggplotLineplot} gives the number of motor vehicle thefts in the city of Chicago by day of the week.

\begin{figure}[h!]
	 \caption{Line graph of the number of motor vehicle thefts in the city of Chicago by day of the week.}
	\center{\includegraphics[width=3.5in]{ggplotLineplot.pdf}}
	\label{fig:ggplotLineplot}
\end{figure}

This plot helps us easily observe that the number of motor vehicle thefts in Chicago (on average) is higher on Friday and lower on Sunday. If we want to also visualize how crime changes with the hour of the day, a heat map is a useful visualization.





\subsection*{Heat Maps}

A great way to visualize frequency data on two attributes (like the frequency of crime according to the day of the week and the hour of the day) is by using a heat map. A heat map creates a grid in two dimensions, with one attribute on the x-axis and the other on the y-axis. There is a square in the grid for every possible pair of the two attributes. For example, in our crime plot, we will have $7 \times 24 = 168$ squares in our grid, one for each hour of each day. 

To see this more concretely, Figure \ref{fig:ggplotHeatmap} shows a heat map for motor vehicle thefts in the city of Chicago. The legend on the right helps us understand the plot. The darker the color is, the more motor vehicle thefts in that hour and day.

\begin{figure}[h!]
	 \caption{A heat map of the number of motor vehicle thefts in the city of Chicago by day of the week and hour of the day. The regions that are the most blue have had the highest number of motor vehicle thefts historically.}
	\center{\includegraphics[width=3.5in]{ggplotHeatmap.pdf}}
	\label{fig:ggplotHeatmap}
\end{figure} 

It is common to change the color scheme of a heat map depending on the application. For example, crime maps are often shown with a red color scheme (like the ones shown here), to give the crime ``hot spots.''



\subsubsection*{Geographical Maps}

Heat maps are often shown on a geographical map to visualize an attribute based on location. For example, Figure \ref{fig:ChicagoHeatMap} shows the number of motor vehicle thefts across the city of Chicago. This is a great visualization for crime ``hot spots,'' and is frequently used in policing to visualize crime trends. 

\begin{figure}[h!]
	 \caption{Heat map of the number of motor vehicle thefts in the city of Chicago by geographic location.}
	\center{\includegraphics[width=3in]{ChicagoHeatMap.pdf}}
	\label{fig:ChicagoHeatMap}
\end{figure} 


\subsection*{United States Map}

A common visualization used by organizations like the Centers for Disease Control (CDC) and the World Health Organization (WHO) is a country or world map, with the different states or countries colored according to some attribute, like obesity, high school graduation, or unemployment. 

As an example, Figure \ref{fig:USmap} gives an unemployment map of the United States from 2009, during the peak of the Great Recession. By looking at this map, we can easily see that some states had significantly higher unemployment rates than others in 2009. 

\begin{figure}[h!]
	 \caption{A heat map on a map of the United States, showing the unemployment rate in 2009.}
	\center{\includegraphics[width=4in]{UnitedStatesUnemployment.pdf}}
	\label{fig:USmap}
\end{figure} 

Visualizations can be used to better understand data, but they can also lead to misconceptions about the data if used incorrectly. We do not want to conclude from this map that Michigan was hit the hardest by the Great Recession without comparing the unemployment rates to those in 2005. When used appropriately, visualizations can be an incredibly powerful way to understand and describe data.


%Add other visualizations??


\section{Linear and Integer Optimization}
\label{sec:Optimization}



Linear and integer optimization are methods used to select optimal decisions. The key components of an optimization model are the decision variables, the objective, and the constraints.




The decision variables in an optimization model represent the choices faced by the decision maker. In a linear optimization model, they can take continuous values in a range. Examples are the number of gallons of crude oil to use to produce gasoline, the number of bushels of vegetables to produce in a given season, and the intensity of beams in radiation therapy. In an integer optimization model, the decision variables are constrained to take integer values (..., -3, -2, -1, 0, 1, 2, 3, ...) or binary values (0 or 1). Examples of integer or binary decision variables are whether or not to invest in a stock, assigning nurses to shifts, the number of new machines to purchase in a factory, and the number of items to stock in a store. Ultimately, an optimization model is used to find the optimal values of these decision variables.






The objective in a linear or integer optimization model is to maximize or minimize a linear function of the decision variables. Examples are to maximize total revenue, to minimize total cost, or to maximize social welfare. This is the ultimate goal of the optimization model. 



The constraints in a linear or integer optimization model are linear equations of the decision variables that must be satisfied by the solution of the model. They often represent feasibility problems in the model; if the constraints are not met, the solution will not work. Examples of possible constraints are to not sell more of a product than the amount that was produced and to not exceed the estimated demand. 

If the constraints and/or the objective are not linear, then the optimization model is called {\it nonlinear}. There are many different types of optimization models, but linear and integer optimization models are some of the most practical and widely used types of optimization models. For more technical details, see the references in Section \ref{sec:MethodsNotesSources}.


\subsection*{The Formulation}

Optimization models are typically written as a set of equations. As an example, here is a simple optimization model:
\begin{align*}
\text{maximize~~ } &617x_1 + 238x_2 \\
\text{subject to~~ } &x_1 + x_2 \leq 166 \\
 &x_1 \leq 100 \\
 &x_2 \leq 150 \\
 &x_1 \geq 0 \\
 &x_2 \geq 0.
\end{align*}

\noindent This is a small example of an airline optimization problem. The airline is trying to decide how many seats to offer at the regular price of $\$617$ ($x_1$) and how many seats to offer at the discount price of $\$238$ ($x_2$) to maximize total revenue. The capacity of the airplane is 166 seats, so they do not want to offer more seats than 166 ($x_1 + x_2 \leq 166$). Additionally, they have estimated the demand of regular seats to be 100, and the demand of discount seats to be 150. They do not want to offer more seats than the demand, since additional seats are unlikely to be purchased ($x_1 \leq 100$ and $x_2 \leq 150$). Lastly, the airline can not sell a negative number of seats, so both decision variables are constrained to be non-negative ($x_1 \geq 0$ and $x_2 \geq 0$). 

 The optimal solution for this problem is $x_1 = 100$ and $x_2 = 66$, with an objective value of $\$617 \times 100 + \$238 \times 66 = \$$77,408. Why? The airline would like to sell as many regular seats as possible, since the revenue from a regular seat is higher than that of a discount seat. So, they sell the maximum number of regular seats that they can, according to the constraints. The remaining seats are offered as discount seats. 

While this problem can be solved by inspection, real-world optimization problems are typically much larger. Imagine this problem with hundreds of different flights and tens of different types of tickets. Additionally, the problem gets even more complex when connecting flights are taken into account. 

To solve optimization models in practice, there are many different commercial and open-source solvers. Small optimization models can be solved with spreadsheet software, which is a very intuitive way of setting up and solving a model. Larger problems require more advanced software, and there are many options available. For more information, see the software manuals in the Online Companion.



\subsection*{Sensitivity Analysis}

An important step in solving an optimization model is to perform {\it sensitivity analysis}. The solution to the optimization model is often sensitive to some of the parameter values. For example, if we increase the capacity of the airplane in our simple example, will the optimal solution change? How about if the demand for regular seats increases? Or the demand for discount seats increases? 

First, let us think about what would happen if the demand for discount seats increases. Since the current optimal solution does not even meet the current demand for discount seats, this will not change the optimal solution at all! The discount demand could even decrease all the way down to 66 without affecting the optimal solution. This is not true for the regular demand. If the regular demand increases, we will want to sell an additional regular seat instead of a discount seat. For each additional unit of regular demand, this will increase our optimal objective function value by $\$617 - \$238 = \$379$, since we are selling a regular ticket instead of a discount ticket. 

 This motivates the definition of the {\it shadow price} of a constraint for a linear optimization model, which is the marginal revenue of increasing the right hand side of the constraint by one. So the shadow price of the discount demand constraint is $\$0$, while the shadow price of the regular demand constraint is $\$379$. However, these values only hold if the constraint is increased to a certain level. Think about what would happen if the regular demand increased by 67. For the first 66 units of increase, the shadow price is $\$379$. But then the capacity of the airplane is full, and any additional increase in regular demand does not matter since there are no more seats left to sell. So the shadow price for the regular demand constraint is $\$379$, with an {\it allowable increase} of 66. 
 

While exploratory sensitivity analysis (changing parameter values and resolving the model) can be done for any type of optimization model, shadow prices are particularly useful for linear optimization models. For more technical details, see the references in Section \ref{sec:MethodsNotesSources}. 







\section{Notes and Sources}
\label{sec:MethodsNotesSources}

For a more mathematically rigorous treatment of the statistical methods described here, we refer you to ``An Introduction to Statistical Learning'' by James et al. \cite{StatisticalLearning}. For a more mathematically rigorous treatment of the optimization methods described here, we refer you to ``Introduction to Linear Optimization'' by Bertsimas and Tsitsiklis \cite{LinearOpt}.

For additional resources describing data mining methods for business, we refer you to ``Data Mining for Business Intelligence'' by Shmueli et al. \cite{DataMining}, and for additional resources describing regression, simulation, and optimization methods for business, we refer you to ``Data, Models, and Decisions'' by Bertsimas and Freund \cite{DMD}. 

\begin{enumerate}[label={\bf \ref{chap:Methods}.\arabic*.}]
\setlength{\itemindent}{0.5em}
\item The wine quality data referenced here comes from the Liquid Assets website (\url{http://www.liquidasset.com/winedata.html}).
\setcounter{enumi}{4}
\item The World Health Organization (WHO) data used for the scatterplots comes from the Global Health Observatory Data Repository (\url{http://apps.who.int/gho/data/?theme=main}), the Chicago crime data used for the line graph and heat maps comes from the City of Chicago website (\url{http://www.cityofchicago.org}), and the unemployment data comes from the United States Bureau of Labor Statistics (\url{http://www.bls.gov/}).
\end{enumerate}




